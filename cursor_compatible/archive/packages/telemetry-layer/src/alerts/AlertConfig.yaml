# Noderr Protocol - Alert Configuration
# Comprehensive alerting rules for all critical metrics

groups:
  - name: model_drift_alerts
    interval: 30s
    rules:
      - alert: HighModelDrift
        expr: noderr_ml_drift_score > 20
        for: 5m
        labels:
          severity: warning
          channel: "#ai-alerts"
        annotations:
          summary: "ML model drift detected"
          description: "Model {{ $labels.model }} has drift score of {{ $value }}% (threshold: 20%)"
          
      - alert: CriticalModelDrift
        expr: noderr_ml_drift_score > 30
        for: 2m
        labels:
          severity: critical
          channel: "#ai-alerts"
        annotations:
          summary: "Critical ML model drift"
          description: "Model {{ $labels.model }} has critical drift score of {{ $value }}% - immediate action required"

  - name: execution_alerts
    interval: 10s
    rules:
      - alert: HighExecutionLatency
        expr: histogram_quantile(0.99, rate(noderr_execution_latency_ms_bucket[5m])) > 2
        for: 3m
        labels:
          severity: warning
          channel: "#infra-alerts"
        annotations:
          summary: "High execution latency detected"
          description: "P99 latency is {{ $value }}ms for venue {{ $labels.venue }} (threshold: 2ms)"
          
      - alert: LowFillRate
        expr: noderr_fill_rate_percent < 90
        for: 5m
        labels:
          severity: warning
          channel: "#trading-alerts"
        annotations:
          summary: "Low order fill rate"
          description: "Fill rate dropped to {{ $value }}% (threshold: 90%)"
          
      - alert: HighSlippage
        expr: histogram_quantile(0.95, rate(noderr_slippage_bps_bucket[5m])) > 10
        for: 5m
        labels:
          severity: warning
          channel: "#trading-alerts"
        annotations:
          summary: "High execution slippage"
          description: "P95 slippage is {{ $value }} bps (threshold: 10 bps)"

  - name: risk_alerts
    interval: 30s
    rules:
      - alert: HighDrawdown
        expr: noderr_drawdown_percent > 15
        for: 1m
        labels:
          severity: critical
          channel: "#risk-alerts"
        annotations:
          summary: "High portfolio drawdown"
          description: "Drawdown reached {{ $value }}% (threshold: 15%)"
          runbook_url: "https://docs.noderr.io/runbooks/high-drawdown"
          
      - alert: VaRBreach
        expr: noderr_var_usd < -100000
        for: 5m
        labels:
          severity: warning
          channel: "#risk-alerts"
        annotations:
          summary: "VaR threshold breached"
          description: "VaR at {{ $value }} USD (threshold: -$100k)"
          
      - alert: HighVolatility
        expr: noderr_realized_volatility > 0.5
        for: 10m
        labels:
          severity: warning
          channel: "#risk-alerts"
        annotations:
          summary: "High portfolio volatility"
          description: "Realized volatility at {{ $value | humanizePercentage }} (threshold: 50%)"

  - name: pnl_alerts
    interval: 60s
    rules:
      - alert: LargePnLDeviation
        expr: |
          abs(noderr_pnl_24h{mode="live"} - noderr_pnl_24h{mode="backtest"}) 
          / abs(noderr_pnl_24h{mode="backtest"}) > 0.1
        for: 10m
        labels:
          severity: warning
          channel: "#quant-checks"
        annotations:
          summary: "P&L deviation from backtest"
          description: "Live P&L deviates {{ $value | humanizePercentage }} from backtest (threshold: 10%)"
          
      - alert: ConsecutiveLosses
        expr: noderr_pnl_24h < 0 and increase(noderr_pnl_24h[3d]) < 0
        for: 1h
        labels:
          severity: warning
          channel: "#trading-alerts"
        annotations:
          summary: "Consecutive daily losses"
          description: "System has losses for 3 consecutive days"

  - name: system_alerts
    interval: 15s
    rules:
      - alert: HighErrorRate
        expr: noderr_error_rate_percent > 5
        for: 3m
        labels:
          severity: critical
          channel: "#infra-alerts"
        annotations:
          summary: "High system error rate"
          description: "Error rate at {{ $value }}% (threshold: 5%)"
          
      - alert: ModuleDown
        expr: up{job="noderr"} == 0
        for: 1m
        labels:
          severity: critical
          channel: "#infra-alerts"
        annotations:
          summary: "Module {{ $labels.module }} is down"
          description: "Module {{ $labels.module }} has been down for more than 1 minute"
          
      - alert: HighMemoryUsage
        expr: noderr_memory_usage_bytes / (1024*1024*1024) > 2
        for: 5m
        labels:
          severity: warning
          channel: "#infra-alerts"
        annotations:
          summary: "High memory usage"
          description: "Module {{ $labels.module }} using {{ $value }}GB memory"
          
      - alert: HighCPUUsage
        expr: noderr_cpu_usage_percent > 80
        for: 5m
        labels:
          severity: warning
          channel: "#infra-alerts"
        annotations:
          summary: "High CPU usage"
          description: "Module {{ $labels.module }} at {{ $value }}% CPU"

  - name: alpha_alerts
    interval: 30s
    rules:
      - alert: LowAlphaHitRate
        expr: noderr_alpha_hit_rate < 70
        for: 15m
        labels:
          severity: warning
          channel: "#quant-checks"
        annotations:
          summary: "Low alpha hit rate"
          description: "Alpha hit rate dropped to {{ $value }}% (threshold: 70%)"
          
      - alert: SharpeRatioDrop
        expr: noderr_sharpe_ratio < 2
        for: 30m
        labels:
          severity: warning
          channel: "#quant-checks"
        annotations:
          summary: "Sharpe ratio below threshold"
          description: "Sharpe ratio at {{ $value }} (threshold: 2.0)"

  - name: incident_management
    interval: 60s
    rules:
      - alert: MultipleAlertsActive
        expr: sum(ALERTS{severity="critical"}) > 3
        for: 5m
        labels:
          severity: critical
          channel: "#incident-response"
        annotations:
          summary: "Multiple critical alerts active"
          description: "{{ $value }} critical alerts are currently active - potential system-wide issue"
          
      - alert: AlertStorm
        expr: rate(ALERTS[5m]) > 1
        for: 5m
        labels:
          severity: warning
          channel: "#infra-alerts"
        annotations:
          summary: "Alert storm detected"
          description: "Alert rate is {{ $value }} per second - possible monitoring issue"

# Alert routing configuration
route:
  group_by: ['alertname', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: 'default'
  routes:
    - match:
        severity: critical
      receiver: 'pagerduty'
      continue: true
    - match:
        channel: '#ai-alerts'
      receiver: 'ai-team'
    - match:
        channel: '#infra-alerts'
      receiver: 'infra-team'
    - match:
        channel: '#risk-alerts'
      receiver: 'risk-team'
    - match:
        channel: '#trading-alerts'
      receiver: 'trading-team'
    - match:
        channel: '#quant-checks'
      receiver: 'quant-team'

receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://alertmanager-webhook:9093/alerts'
        
  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: '<PAGERDUTY_SERVICE_KEY>'
        severity: 'critical'
        
  - name: 'ai-team'
    slack_configs:
      - api_url: '<SLACK_WEBHOOK_URL>'
        channel: '#ai-alerts'
        title: 'AI/ML Alert'
        
  - name: 'infra-team'
    slack_configs:
      - api_url: '<SLACK_WEBHOOK_URL>'
        channel: '#infra-alerts'
        title: 'Infrastructure Alert'
        
  - name: 'risk-team'
    slack_configs:
      - api_url: '<SLACK_WEBHOOK_URL>'
        channel: '#risk-alerts'
        title: 'Risk Alert'
    email_configs:
      - to: 'risk-team@noderr.io'
        from: 'alerts@noderr.io'
        smarthost: 'smtp.gmail.com:587'
        
  - name: 'trading-team'
    slack_configs:
      - api_url: '<SLACK_WEBHOOK_URL>'
        channel: '#trading-alerts'
        title: 'Trading Alert'
        
  - name: 'quant-team'
    slack_configs:
      - api_url: '<SLACK_WEBHOOK_URL>'
        channel: '#quant-checks'
        title: 'Quant Alert'

# Inhibition rules to prevent alert flooding
inhibit_rules:
  - source_match:
      severity: 'critical'
      alertname: 'ModuleDown'
    target_match:
      severity: 'warning'
    equal: ['module']
    
  - source_match:
      severity: 'critical'
      alertname: 'HighDrawdown'
    target_match:
      severity: 'warning'
      alertname: 'VaRBreach' 